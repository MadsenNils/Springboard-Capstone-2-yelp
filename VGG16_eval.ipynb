{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit GPU memory usage by tensorflow\n",
    "config = K.tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "K.tensorflow_backend.set_session(K.tf.Session(config=config))\n",
    "\n",
    "def reset_session():\n",
    "    K.get_session().close()\n",
    "    K.set_session(K.tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9000 images belonging to 5 classes.\n",
      "Found 1000 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# define paths to image directories\n",
    "train_path = 'sampled_photos/train'\n",
    "valid_path = 'sampled_photos/val'\n",
    "\n",
    "# create image data generators to feed the model from image directories\n",
    "target_size = (224, 224)\n",
    "batch_size = 8\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=target_size, \n",
    "                                                         batch_size=batch_size)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path, \n",
    "                                                         target_size=target_size,\n",
    "                                                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract true labels from generator\n",
    "y_true = []\n",
    "for _ in range(len(valid_batches)):\n",
    "    x, y = valid_batches.next()\n",
    "    y_true.extend(y)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('vgg16_lr_0-0001_1.h5')\n",
    "y_pred_prob = model.predict_generator(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.DataFrame(y_pred_prob).iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e3383f3f812b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "y_pred = []\n",
    "y_pred_prob = pd.DataFrame(y_pred_prob)\n",
    "for row in y_pred_prob.iterrows():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_VGG16(width, new_weights=False, trainable=False, learning_rate=0.0001):\n",
    "    '''\n",
    "    Builds a modified version of the VGG16 model for transfer learning\n",
    "    \n",
    "    Parameters:\n",
    "    width(int) - number of nodes present in each of the two new FC layers after\n",
    "        the convolutional layers\n",
    "    new_weights(bool) - whether to reinitialize the weights in the VGG16\n",
    "        convolutional layers\n",
    "    trainable(bool) - whether to allow updating of convolutional weights\n",
    "    \n",
    "    Returns:\n",
    "    Compiled keras functional API model object\n",
    "    '''\n",
    "    \n",
    "    if new_weights == True:\n",
    "        weights = None\n",
    "    else:\n",
    "        weights = 'imagenet'\n",
    "    \n",
    "    # import only the convolutional layers of VGG16\n",
    "    base_model = VGG16(include_top=False, \n",
    "                       weights=weights, \n",
    "                       input_shape=(224, 224, 3))\n",
    "    \n",
    "    if trainable == False:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # add two FC layers to end of convolutional layers\n",
    "    inputs = base_model.output\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(width, activation='relu')(x)\n",
    "    x = Dense(width, activation='relu')(x)\n",
    "    preds = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    # compile model\n",
    "    model = Model(inputs=base_model.inputs, outputs=preds)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate, decay=0.1), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_duplicate(width, n_epochs=10, new_weights=False, trainable=False, learning_rate=0.0001):\n",
    "    '''\n",
    "    Runs two initialization and training passes for each model variation\n",
    "    \n",
    "    Returns the training history\n",
    "    '''\n",
    "    from datetime import datetime\n",
    "    \n",
    "    filename = 'vgg16_lr_{}_'.format(str(learning_rate).replace('.', '-'))\n",
    "    \n",
    "    datetime_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('{} - Starting training for {}'.format(datetime_now, filename + '1'))\n",
    "    \n",
    "    # define callbacks for model fitting\n",
    "    checkpointer = ModelCheckpoint(filename + '1.h5', \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=0, \n",
    "                                   save_best_only=True)\n",
    "    \n",
    "    # build first model\n",
    "    model_1 = build_VGG16(width, new_weights=new_weights, trainable=trainable, learning_rate=learning_rate)\n",
    "    \n",
    "    # fit the model to the training data\n",
    "    history_1 = model_1.fit_generator(train_batches, \n",
    "                                      steps_per_epoch=9000/batch_size, \n",
    "                                      validation_data=valid_batches, \n",
    "                                      validation_steps=1000/batch_size, \n",
    "                                      epochs=n_epochs, \n",
    "                                      callbacks=[checkpointer], \n",
    "                                      verbose=0)\n",
    "    \n",
    "    datetime_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('{} - Starting training for {}'.format(datetime_now, filename + '2'))\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filename + '2.h5', \n",
    "                                   monitor='val_loss', \n",
    "                                   verbose=0, \n",
    "                                   save_best_only=True)\n",
    "    \n",
    "    # build second model\n",
    "    model_2 = build_VGG16(width, new_weights=new_weights, trainable=trainable, learning_rate=learning_rate)\n",
    "    \n",
    "    # fit the model to the training data\n",
    "    history_2 = model_2.fit_generator(train_batches, \n",
    "                                      steps_per_epoch=9000/batch_size, \n",
    "                                      validation_data=valid_batches, \n",
    "                                      validation_steps=1000/batch_size, \n",
    "                                      epochs=n_epochs, \n",
    "                                      callbacks=[checkpointer], \n",
    "                                      verbose=0)\n",
    "    \n",
    "    def avg_metric(metric):\n",
    "        metric_1 = np.array(history_1.history[metric])\n",
    "        metric_2 = np.array(history_2.history[metric])\n",
    "        return (metric_1 + metric_2)/2\n",
    "    \n",
    "    # determine average metrics of the two runs\n",
    "    acc = avg_metric('acc')\n",
    "    loss = avg_metric('loss')\n",
    "    val_acc = avg_metric('val_acc')\n",
    "    val_loss = avg_metric('val_loss')\n",
    "    \n",
    "    # plot loss history\n",
    "    plot_df = tuple([loss, val_loss])\n",
    "    plot_df = np.column_stack(plot_df)\n",
    "    plot_df = pd.DataFrame(plot_df, columns=('training', 'validation'))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    plot_df.plot(ax=ax)\n",
    "    plt.title('Loss over epochs: {}'.format(filename))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xticks(range(0, len(loss)))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'acc': acc, 'loss':loss, 'val_acc':val_acc, 'val_loss':val_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['0.001' '0.0005'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dc8021537774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# drop conditions that didn't converge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0.001'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0.0005'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3697\u001b[1;33m                                            errors=errors)\n\u001b[0m\u001b[0;32m   3698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3143\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   4402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4403\u001b[0m                 raise KeyError(\n\u001b[1;32m-> 4404\u001b[1;33m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[0;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['0.001' '0.0005'] not found in axis\""
     ]
    }
   ],
   "source": [
    "def plot_metric(metric, histories):\n",
    "    # extract metric of interest\n",
    "    hist_df = pd.DataFrame()\n",
    "    for i, history in histories.iterrows():\n",
    "        hist_df[i] = history[metric]\n",
    "\n",
    "    # plot history\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    hist_df.plot(ax=ax)\n",
    "    plt.title('Performance over epochs: {}'.format(metric))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(range(0, len(hist_df.iloc[:, 1])))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "# load history from json\n",
    "histories = pd.read_json('VGG16_learnrate_comparison_history.json', convert_axes=False)\n",
    "\n",
    "# drop conditions that didn't converge\n",
    "histories.drop(['0.001', '0.0005'], inplace=True)\n",
    "    \n",
    "plot_metric('acc', histories)\n",
    "plot_metric('loss', histories)\n",
    "plot_metric('val_acc', histories)\n",
    "plot_metric('val_loss', histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00001</th>\n",
       "      <th>0.00005</th>\n",
       "      <th>0.0001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>acc</td>\n",
       "      <td>[0.8700555556, 0.9335, 0.9482777778, 0.9562222...</td>\n",
       "      <td>[0.8875000000000001, 0.9257777778, 0.9365, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>loss</td>\n",
       "      <td>[0.4666569245, 0.2092030527, 0.1665932806, 0.1...</td>\n",
       "      <td>[0.3736501131, 0.2177322939, 0.1846553085, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_acc</th>\n",
       "      <td>val_acc</td>\n",
       "      <td>[0.891, 0.896, 0.8995000000000001, 0.904500000...</td>\n",
       "      <td>[0.9075000000000001, 0.9125000000000001, 0.917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_loss</th>\n",
       "      <td>val_loss</td>\n",
       "      <td>[0.3504745495, 0.3282947483, 0.3175205604, 0.3...</td>\n",
       "      <td>[0.2900064073, 0.2715007702, 0.2645731816, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0.00001                                            0.00005  \\\n",
       "acc            acc  [0.8700555556, 0.9335, 0.9482777778, 0.9562222...   \n",
       "loss          loss  [0.4666569245, 0.2092030527, 0.1665932806, 0.1...   \n",
       "val_acc    val_acc  [0.891, 0.896, 0.8995000000000001, 0.904500000...   \n",
       "val_loss  val_loss  [0.3504745495, 0.3282947483, 0.3175205604, 0.3...   \n",
       "\n",
       "                                                     0.0001  \n",
       "acc       [0.8875000000000001, 0.9257777778, 0.9365, 0.9...  \n",
       "loss      [0.3736501131, 0.2177322939, 0.1846553085, 0.1...  \n",
       "val_acc   [0.9075000000000001, 0.9125000000000001, 0.917...  \n",
       "val_loss  [0.2900064073, 0.2715007702, 0.2645731816, 0.2...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
